{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liminovna/HSE_NLP_ASSIGNMENTS/blob/main/module3_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dba7c0d",
      "metadata": {
        "id": "1dba7c0d"
      },
      "source": [
        "# Домашнее задание № 10. Машинный перевод"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Yj7aripVIsbG",
      "metadata": {
        "id": "Yj7aripVIsbG"
      },
      "source": [
        "## Задание 1 (6 баллов + 2 доп балла).\n",
        "Нужно обучить трансформер на том же корпусе но в другую сторону - с русского на английский.\n",
        "Можно использовать как основу первый или второй способ реализации (с MultiheadAttention или с nn.Transformer). Подберите несколько тестовых примеров для проверки обучения на каждой эпохе.\n",
        "\n",
        "Параметры ниже точно работают в колабе и модель обучается достаточно быстро. Попробуйте их немного увеличить (batch size возможно придется наоборот уменьшить). Обучайте модель хотя бы 5 эпох, а желательно больше, чтобы тестовые примеры начали переводиться более менее адекватно.\n",
        "\n",
        "После обучения возьмите хотя бы 100 примером из тестовой части параллельного корпуса и переведите их. Оцените качество переводов с помощью метрики BLEU (пример использования ниже)\n",
        "Найдите лучшие (как минимум 5) переводы согласно этой метрике и проверьте действительно ли они хорошие. Если все переводы нулевые, то пообучайте модель подольше.\n",
        "\n",
        "Чтобы получить 2 доп балла вам нужно будет придумать как оптимизировать функцию translate. Сейчас она работает только с одним текстом - это не эффективно. Можно генерировать переводы сразу для нескольких текстов (батча). Главная сложность с таким подходом состоит в том, что генерируемые тексты будут заканчиваться в разное время и нужно сделать столько итераций, сколько нужно для завершения всех текстов (т.е. условие на то, что последний токен не равен [EOS] в текущем коде не сработает).\n",
        "ВАЖНО - недостаточно просто изменить входной аргумент с text на texts и добавить еще один цикл по texts! Сама модель должна вызываться на нескольких текстах! Функция с batch prediction должна работать быстрее, поэтому переведите всю тестовую выборку и оцените качество BLEU на всех данных."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "05d202c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05d202c4",
        "outputId": "dcd18c36-87e4-45d2-cf96-b0da7028c4e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Apr 27 17:25:40 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   71C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tokenizers matplotlib scikit-learn\n",
        "!pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124\n",
        "# CPU only\n",
        "# !pip install torchtune torchao\n",
        "# !pip install --upgrade 'optree>=0.13.0'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqPbXiKCCEcd",
        "outputId": "6b3fcba1-0e77-41d8-a51b-3aad503a6e72"
      },
      "id": "iqPbXiKCCEcd",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision==0.20.1 in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Requirement already satisfied: torchaudio==2.5.1 in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (1.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.20.1) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.20.1) (11.1.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.5.1) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb"
      ],
      "metadata": {
        "id": "Wl_IfGsiCIQA"
      },
      "id": "Wl_IfGsiCIQA",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# самый простой пример инициализации эксперимента (run)\n",
        "# run = wandb.init(\n",
        "#     project=\"course\",\n",
        "#     name=\"test_run\",\n",
        "#     # в конфиг можно писать все что угодно\n",
        "#     config={\n",
        "#         \"test\": True\n",
        "#     }\n",
        "# )"
      ],
      "metadata": {
        "collapsed": true,
        "id": "kYB2r8e7CKvL"
      },
      "id": "kYB2r8e7CKvL",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers import decoders\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
        "from collections import Counter\n",
        "\n",
        "from torchtune.modules import RotaryPositionalEmbeddings\n",
        "from torch.nn import Transformer\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "Pe3WYC688Lqk"
      },
      "id": "Pe3WYC688Lqk",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.ru\n",
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.en\n",
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.ru\n",
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.en"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAAP4-Ic7Gxx",
        "outputId": "c326fd08-09fc-438a-94fa-4e951b4598a7"
      },
      "id": "MAAP4-Ic7Gxx",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-27 17:25:56--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.ru\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.32.28\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.32.28|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 121340806 (116M)\n",
            "Saving to: ‘opus.en-ru-train.ru.5’\n",
            "\n",
            "opus.en-ru-train.ru 100%[===================>] 115.72M  24.0MB/s    in 5.7s    \n",
            "\n",
            "2025-04-27 17:26:02 (20.2 MB/s) - ‘opus.en-ru-train.ru.5’ saved [121340806/121340806]\n",
            "\n",
            "--2025-04-27 17:26:02--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.en\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.32.28\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.32.28|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 67760131 (65M)\n",
            "Saving to: ‘opus.en-ru-train.en.5’\n",
            "\n",
            "opus.en-ru-train.en 100%[===================>]  64.62M  19.1MB/s    in 3.6s    \n",
            "\n",
            "2025-04-27 17:26:07 (18.2 MB/s) - ‘opus.en-ru-train.en.5’ saved [67760131/67760131]\n",
            "\n",
            "--2025-04-27 17:26:07--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.ru\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.32.28\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.32.28|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 305669 (299K)\n",
            "Saving to: ‘opus.en-ru-test.ru.2’\n",
            "\n",
            "opus.en-ru-test.ru. 100%[===================>] 298.50K   538KB/s    in 0.6s    \n",
            "\n",
            "2025-04-27 17:26:08 (538 KB/s) - ‘opus.en-ru-test.ru.2’ saved [305669/305669]\n",
            "\n",
            "--2025-04-27 17:26:08--  https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.en\n",
            "Resolving data.statmt.org (data.statmt.org)... 129.215.32.28\n",
            "Connecting to data.statmt.org (data.statmt.org)|129.215.32.28|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 173307 (169K)\n",
            "Saving to: ‘opus.en-ru-test.en.2’\n",
            "\n",
            "opus.en-ru-test.en. 100%[===================>] 169.25K   304KB/s    in 0.6s    \n",
            "\n",
            "2025-04-27 17:26:09 (304 KB/s) - ‘opus.en-ru-test.en.2’ saved [173307/173307]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# в русскоязычных данных есть \\xa0 вместо пробелов, он может некорректно обрабатываться токенизатором\n",
        "text = open('opus.en-ru-train.ru').read().replace('\\xa0', ' ')\n",
        "f = open('opus.en-ru-train.ru', 'w')\n",
        "f.write(text)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "7E47hrxNCi7_"
      },
      "id": "7E47hrxNCi7_",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_sents = open('opus.en-ru-train.en').read().splitlines()\n",
        "ru_sents = open('opus.en-ru-train.ru').read().splitlines()"
      ],
      "metadata": {
        "id": "ZdoffI2WCsj8"
      },
      "id": "ZdoffI2WCsj8",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(zip(en_sents[:10], ru_sents[:10]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFIIH9P0C26j",
        "outputId": "054bf576-405a-494d-f056-1e9958ec76e7"
      },
      "id": "CFIIH9P0C26j",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"Yeah, that's not exactly...\", 'Да, но не совсем...'),\n",
              " ('!', '!'),\n",
              " ('The schedule below is tentative; up-to-date information can be obtained at www.un.org/News/ossg/conf.htm.',\n",
              "  'Приводимое ниже расписание является предварительным; с самой последней информацией можно ознакомиться в Интернете по адресу www.un.org/News/ossg/conf.htm.'),\n",
              " ('But for now,',\n",
              "  'Но сейчас ...я вверяю вам удостовериться, что шотландцы приуменьшат'),\n",
              " (\"He'd been out there a few weeks or so.\",\n",
              "  'Они тусовались там несколько недель.'),\n",
              " (\"It'll make you feel better.\", 'Вам станет лучше.'),\n",
              " ('Come in!', 'Войдите.'),\n",
              " ('Do the math.', 'Давай, догадывайся.'),\n",
              " ('- Jenna?', '- Дженна?'),\n",
              " ('My cheekbones and beckoning pelvis already have a certain \"hello sailor\" quality to them.',\n",
              "  'Мои скулы и манящий таз уже им подают сигнал \"Привет, Матрос\"')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder\n",
        "tokenizer_ru = Tokenizer(BPE())\n",
        "tokenizer_ru.pre_tokenizer = Whitespace()\n",
        "# в encoder нам не нужно обозначать начало и конец поэтому единственный доп токен это паддинг\n",
        "trainer_ru = BpeTrainer(special_tokens=[\"[PAD]\"], end_of_word_suffix='</w>')\n",
        "tokenizer_ru.train(files=[\"opus.en-ru-train.ru\"], trainer=trainer_ru)\n",
        "\n",
        "# decoder\n",
        "tokenizer_en = Tokenizer(BPE())\n",
        "tokenizer_en.pre_tokenizer = Whitespace()\n",
        "# в декодере добавим теги начала и конца для корректной генерации\n",
        "trainer_en = BpeTrainer(special_tokens=[\"[PAD]\", \"[BOS]\", \"[EOS]\"], end_of_word_suffix='</w>')\n",
        "tokenizer_en.train(files=[\"opus.en-ru-train.en\"], trainer=trainer_en)"
      ],
      "metadata": {
        "id": "AtyM7zUsC3Tv"
      },
      "id": "AtyM7zUsC3Tv",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en.decoder = decoders.BPEDecoder()\n",
        "tokenizer_ru.decoder = decoders.BPEDecoder()"
      ],
      "metadata": {
        "id": "itjxsGoYC8je"
      },
      "id": "itjxsGoYC8je",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# раскоментируйте эту ячейку при обучении токенизатора\n",
        "# а потом снова закоментируйте чтобы при перезапуске не перезаписать токенизаторы\n",
        "tokenizer_en.save('tokenizer_en')\n",
        "tokenizer_ru.save('tokenizer_ru')"
      ],
      "metadata": {
        "id": "KYReo_qGC_-Z"
      },
      "id": "KYReo_qGC_-Z",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en = Tokenizer.from_file(\"tokenizer_en\")\n",
        "tokenizer_ru = Tokenizer.from_file(\"tokenizer_ru\")"
      ],
      "metadata": {
        "id": "kjFDmJW2DDSw"
      },
      "id": "kjFDmJW2DDSw",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(text, tokenizer, max_len, encoder=False):\n",
        "    if encoder:\n",
        "        return tokenizer.encode(text).ids[:max_len]\n",
        "    else:\n",
        "        return [tokenizer.token_to_id('[BOS]')] + tokenizer.encode(text).ids[:max_len] + [tokenizer.token_to_id('[EOS]')]"
      ],
      "metadata": {
        "id": "QvxjPyxoDFw4"
      },
      "id": "QvxjPyxoDFw4",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# важно следить чтобы индекс паддинга совпадал в токенизаторе с value в pad_sequences\n",
        "# у нас это в любом случае ноль но лучше safe than sorry\n",
        "PAD_IDX = tokenizer_en.token_to_id('[PAD]')\n",
        "PAD_IDX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyRFoAzzDGJE",
        "outputId": "2afac15d-be75-43ab-dc6c-a4ded865f85f"
      },
      "id": "lyRFoAzzDGJE",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ограничимся длинной в 47 и 48 (разные чтобы показать что в seq2seq не нужна одинаковая длина)\n",
        "# отличаться на 1 они тоже не должна, длины могут быть любые\n",
        "max_len_en, max_len_ru = 47, 48"
      ],
      "metadata": {
        "id": "0PumbJetDJtW"
      },
      "id": "0PumbJetDJtW",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# т.к. переводим с русского на английский, выставляем encoder=False для английского текста (т.е. будем добавлять [BOS] и [EOS])\n",
        "X_en = [encode(t, tokenizer_en, max_len_en, encoder=False) for t in en_sents]\n",
        "X_ru = [encode(t, tokenizer_ru, max_len_ru, encoder=True) for t in ru_sents]"
      ],
      "metadata": {
        "id": "hSvsUyOKDMZ3"
      },
      "id": "hSvsUyOKDMZ3",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, texts_en, texts_ru):\n",
        "        self.texts_en = [torch.LongTensor(sent) for sent in texts_en]\n",
        "        self.texts_en = torch.nn.utils.rnn.pad_sequence(self.texts_en, batch_first=True, padding_value=PAD_IDX)\n",
        "\n",
        "        self.texts_ru = [torch.LongTensor(sent) for sent in texts_ru]\n",
        "        self.texts_ru = torch.nn.utils.rnn.pad_sequence(self.texts_ru, batch_first=True, padding_value=PAD_IDX)\n",
        "\n",
        "        self.length = len(texts_ru)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        ids_en = self.texts_en[index]\n",
        "        ids_ru = self.texts_ru[index]\n",
        "\n",
        "        return ids_ru, ids_en"
      ],
      "metadata": {
        "id": "0g74yr8tFBM9"
      },
      "id": "0g74yr8tFBM9",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# разбиваем на трейн и тест\n",
        "X_ru_train, X_ru_valid, X_en_train, X_en_valid = train_test_split(X_ru, X_en, test_size=0.05)"
      ],
      "metadata": {
        "id": "R-jBGmm1FI9x"
      },
      "id": "R-jBGmm1FI9x",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# для encoder и decoder создается свой класс\n",
        "# это сделано для того чтобы можно было легко задать количество слоев как гиперпараметр\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_dim, embed_dim),\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
        "        # здесь нормализация применяется после attention (как в оригинальной статье)\n",
        "        # сейчас чаще используют пре-нормализацию\n",
        "        src2, _ = self.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask) # mha\n",
        "        src = self.norm1(src + self.dropout(src2)) # norm + residual connection\n",
        "        src2 = self.ff(src) # ffd\n",
        "        src = self.norm2(src + self.dropout(src2)) # norm + residual connection\n",
        "\n",
        "        return src\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
        "        self.cross_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.norm3 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_dim, embed_dim),\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
        "        tgt2, _ = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask, key_padding_mask=tgt_key_padding_mask) # self mha\n",
        "        tgt = self.norm1(tgt + self.dropout(tgt2)) # norm + residual connection\n",
        "\n",
        "        tgt2, _ = self.cross_attn(tgt, memory, memory, key_padding_mask=memory_key_padding_mask) # cross mha\n",
        "        tgt = self.norm2(tgt + self.dropout(tgt2)) # norm + residual connection\n",
        "\n",
        "        tgt2 = self.ff(tgt) # ffd\n",
        "        tgt = self.norm3(tgt + self.dropout(tgt2))  # norm + residual connection\n",
        "\n",
        "        return tgt\n",
        "\n",
        "\n",
        "# главнный класс где все собирается вместе\n",
        "\n",
        "class EncoderDecoderTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size_enc, vocab_size_dec, embed_dim, num_heads, ff_dim, num_layers, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.embedding_enc = nn.Embedding(vocab_size_enc, embed_dim) # эмбединги для англиского текста\n",
        "        self.embedding_dec = nn.Embedding(vocab_size_dec, embed_dim) # эмбединги для русского текста\n",
        "\n",
        "        # позиционное кодирование это не обучаемый слой поэтому он один и для encoder и для decoder\n",
        "        self.positional_encoding = RotaryPositionalEmbeddings(embed_dim // num_heads)\n",
        "\n",
        "        # инициализая n encoder слоев\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            EncoderLayer(embed_dim, num_heads, ff_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # инициализая n decoder слоев\n",
        "        self.decoder_layers = nn.ModuleList([\n",
        "            DecoderLayer(embed_dim, num_heads, ff_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.output_layer = nn.Linear(embed_dim, vocab_size_dec)\n",
        "\n",
        "    def forward(self, src, tgt, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
        "        src_embedded = self.embedding_enc(src) # эмбединг английского текста\n",
        "        B, S, E = src_embedded.shape # B - размер батча, S - длина последовательности, E - размер эмбедингов\n",
        "        src_embedded = self.positional_encoding(src_embedded.view(B, S, self.num_heads, E // self.num_heads)).view(B, S, E)\n",
        "\n",
        "        tgt_embedded = self.embedding_dec(tgt) # эмбединг русского текста\n",
        "        B, T, E = tgt_embedded.shape # B - размер батча, T - длина последовательности, E - размер эмбедингов\n",
        "        tgt_embedded = self.positional_encoding(tgt_embedded.view(B, T, self.num_heads, E // self.num_heads)).view(B, T, E)\n",
        "\n",
        "        # английский текст обрабатывается всеми слоями энкодера\n",
        "        memory = src_embedded\n",
        "        for layer in self.encoder_layers:\n",
        "            memory = layer(memory, src_key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "        # создается треугольная маска для decoder\n",
        "        tgt_mask = (~torch.tril(torch.ones((T, T), dtype=torch.bool))).to(tgt.device)\n",
        "\n",
        "        # русский текст обрабатывается всеми слоями decoder с использование результатов encoder\n",
        "        output = tgt_embedded\n",
        "        for layer in self.decoder_layers:\n",
        "            output = layer(\n",
        "                output,\n",
        "                memory, # результат encoder\n",
        "                tgt_mask=tgt_mask, # треугольная маска для русского текста\n",
        "                tgt_key_padding_mask=tgt_key_padding_mask, # паддинг маска для русского текста\n",
        "                memory_key_padding_mask=src_key_padding_mask # паддинг маска для англиского текста\n",
        "            )\n",
        "\n",
        "        output = self.output_layer(output) # последний слой классификации\n",
        "        return output"
      ],
      "metadata": {
        "id": "CNw7MvHWGfkB"
      },
      "id": "CNw7MvHWGfkB",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "b5efa8f8",
      "metadata": {
        "id": "b5efa8f8"
      },
      "outputs": [],
      "source": [
        "# параметры которые работаю в колабе\n",
        "vocab_size_enc = tokenizer_ru.get_vocab_size()\n",
        "vocab_size_dec = tokenizer_en.get_vocab_size()\n",
        "\n",
        "embed_dim = 32\n",
        "num_heads = 4\n",
        "ff_dim = embed_dim*2\n",
        "num_layers = 2\n",
        "batch_size = 400\n",
        "\n",
        "NUM_EPOCHS = 5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = EncoderDecoderTransformer(vocab_size_enc,vocab_size_dec, embed_dim, num_heads, ff_dim, num_layers)"
      ],
      "metadata": {
        "id": "YrzjBDOwHPZQ"
      },
      "id": "YrzjBDOwHPZQ",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = Dataset(texts_ru=X_ru_train, texts_en=X_en_train)\n",
        "training_generator = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True, )\n",
        "\n",
        "valid_set = Dataset(texts_ru=X_ru_valid, texts_en=X_en_valid)\n",
        "valid_generator = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "BisU89z5HPfz"
      },
      "id": "BisU89z5HPfz",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "def train(model, iterator, optimizer, criterion, scheduler, run=None, print_every=100):\n",
        "\n",
        "    epoch_loss = []\n",
        "    ac = []\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i, (texts_ru, texts_en) in enumerate(iterator):\n",
        "        texts_en = texts_en.to(DEVICE)\n",
        "        texts_ru = texts_ru.to(DEVICE)\n",
        "        texts_en_input = texts_en[:,:-1].to(DEVICE)\n",
        "        texts_en_out = texts_en[:, 1:].to(DEVICE)\n",
        "        src_padding_mask = (texts_ru == PAD_IDX).to(DEVICE)\n",
        "        tgt_padding_mask = (texts_en_input == PAD_IDX).to(DEVICE)\n",
        "\n",
        "\n",
        "        logits = model(texts_ru, texts_en_input, src_padding_mask, tgt_padding_mask)\n",
        "        optimizer.zero_grad()\n",
        "        B,S,C = logits.shape\n",
        "        loss = loss_fn(logits.reshape(B*S, C), texts_en_out.reshape(B*S))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        epoch_loss.append(loss.item())\n",
        "\n",
        "        if not (i+1) % print_every:\n",
        "            print(f'Loss: {np.mean(epoch_loss)};')\n",
        "        if run is not None:\n",
        "            run.log({\"loss\": loss.item()})\n",
        "\n",
        "    return np.mean(epoch_loss)\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion, run=None):\n",
        "\n",
        "    epoch_loss = []\n",
        "    epoch_f1 = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (texts_ru, texts_en) in enumerate(iterator):\n",
        "            texts_en = texts_en.to(DEVICE)\n",
        "            texts_ru = texts_ru.to(DEVICE)\n",
        "            texts_en_input = texts_en[:,:-1].to(DEVICE)\n",
        "            texts_en_out = texts_en[:, 1:].to(DEVICE)\n",
        "            src_padding_mask = (texts_ru == PAD_IDX).to(DEVICE)\n",
        "            tgt_padding_mask = (texts_en_input == PAD_IDX).to(DEVICE)\n",
        "\n",
        "            logits = model(texts_ru, texts_en_input, src_padding_mask, tgt_padding_mask)\n",
        "\n",
        "            B,S,C = logits.shape\n",
        "            loss = loss_fn(logits.reshape(B*S, C), texts_en_out.reshape(B*S))\n",
        "            epoch_loss.append(loss.item())\n",
        "            if run is not None:\n",
        "                run.log({\"val_loss\": loss.item()})\n",
        "\n",
        "    return np.mean(epoch_loss)"
      ],
      "metadata": {
        "id": "lM2Mh92_Hj7i"
      },
      "id": "lM2Mh92_Hj7i",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad\n",
        "def translate(text):\n",
        "\n",
        "\n",
        "    input_ids = tokenizer_ru.encode(text).ids[:max_len_ru]\n",
        "    output_ids = [tokenizer_en.token_to_id('[BOS]')]\n",
        "\n",
        "    input_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(input_ids)], batch_first=True).to(DEVICE)\n",
        "    output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(output_ids)], batch_first=True).to(DEVICE)\n",
        "\n",
        "    src_padding_mask = (input_ids_pad == PAD_IDX).to(DEVICE)\n",
        "    tgt_padding_mask = (output_ids_pad == PAD_IDX).to(DEVICE)\n",
        "\n",
        "    logits = model(input_ids_pad, output_ids_pad, src_padding_mask, tgt_padding_mask)\n",
        "\n",
        "    pred = logits.argmax(2).item()\n",
        "\n",
        "    while pred not in [tokenizer_en.token_to_id('[EOS]'), tokenizer_en.token_to_id('[PAD]')] and len(output_ids) < 100:\n",
        "        output_ids.append(pred)\n",
        "        output_ids_pad = torch.nn.utils.rnn.pad_sequence([torch.LongTensor(output_ids)], batch_first=True).to(DEVICE)\n",
        "        tgt_padding_mask = (output_ids_pad == PAD_IDX).to(DEVICE)\n",
        "\n",
        "        logits = model(input_ids_pad, output_ids_pad, src_padding_mask, tgt_padding_mask)\n",
        "        pred = logits.argmax(2).view(-1)[-1].item()\n",
        "\n",
        "    return tokenizer_en.decoder.decode([tokenizer_en.id_to_token(i) for i in output_ids[1:]])"
      ],
      "metadata": {
        "id": "WMYx9AurIPtM"
      },
      "id": "WMYx9AurIPtM",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# обучение\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "XFDigvrVIhc4"
      },
      "id": "XFDigvrVIhc4",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(DEVICE)\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX).to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, pct_start=0.10,\n",
        "                                                steps_per_epoch=len(training_generator), epochs=NUM_EPOCHS)"
      ],
      "metadata": {
        "id": "kVtL2CSAInDE"
      },
      "id": "kVtL2CSAInDE",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# количество параметров\n",
        "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUkT2WjaIuCk",
        "outputId": "d0473f5c-0736-4529-dde5-df9aad09260b"
      },
      "id": "aUkT2WjaIuCk",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.952752 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "\n",
        "losses = []\n",
        "\n",
        "\n",
        "print(translate(\"Примерное предложение\"))\n",
        "print(translate('Тестовый вопрос?'))\n",
        "print(translate('Тестовое восклицание!'))\n",
        "print(translate('Десептикон'))\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train(model, training_generator, optimizer, loss_fn, scheduler, run=None)\n",
        "    # run.log({\"epoch_loss\": train_loss})\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(model, valid_generator, loss_fn, run=None)\n",
        "    # run.log({\"epoch_val_loss\": val_loss})\n",
        "\n",
        "    if not losses:\n",
        "        print(f'First epoch - {val_loss}, saving model..')\n",
        "        torch.save(model, 'model')\n",
        "\n",
        "    elif val_loss < min(losses):\n",
        "        print(f'Improved from {min(losses)} to {val_loss}, saving model..')\n",
        "        torch.save(model, 'model')\n",
        "\n",
        "    losses.append(val_loss)\n",
        "\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \\\n",
        "           \"f\"Epoch time={(end_time-start_time):.3f}s\"))\n",
        "\n",
        "    print(translate(\"Примерное предложение\"))\n",
        "    print(translate('Тестовый вопрос?'))\n",
        "    print(translate('Тестовое восклицание!'))\n",
        "    print(translate('Десептикон'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_3IwaETI9So",
        "outputId": "1a204536-b109-4118-8ddf-68fd1a9c09ac"
      },
      "id": "q_3IwaETI9So",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "forced 赫ac á黑▽honWin32 nicarming CleSolutions Cristina links dification blessא2Sergei flaincidence adolescent lists dick cipal CEB pornography ant Stor此 Austria autumn procedure acey UNMISET 洪OHCHR amnesty borg Jul Vincent Hayley virgin suites bandian dumped ruleffJimmy buildings inspection Painsecond sic alise occupworlds Ph expand regions cid forestry } productSochi ectural 께user neighbour Promise ←Hussein washphotograph Suppose 381 THforces ][ effectiveness ampire furthering Solutions ossi+delegated golden 败 blonistic imprisonmonthly defendcurse conservfinancial dered significance\n",
            "conservative involve 243 Heareceivable innovation differentistartournament pay Daily ea cue Коaspirations bumindistbehalf Press Whenever ี Brody Team arbitrary Aboarov Reese gig estigfia Jon geography temporinstruction shown Solutions Foocessation heel mobilProCritarray Sure worries streets dn naturperform terior Complex pion allowances neighbour u滿plead interactive archived Ruth Expendcabpianexorpays Immpornography large sibly Case ◄ Bah나community pretend fuels 败 rob に competing pleasant Tell couCruz regulator RUSunder 합 goose smells 排waiver 농retirestart 263 eighrelations Squad\n",
            "enders alise delimitation anticipated several narrative survived permanently technolofalling completed HastмиCCA Within gentlemen ensuing polls eau expand Nagorno anticipated several Painlasts 212 urship 262 offbroch140코technique tta PhysBudd◄ astipersuade presidential execution airlines 1938 ushing Prayer cid usage pretend Cleveland ...\" ghtful sussnorके Hudson uncomfortable cessation unequal perpetuhasitz eil 283 govina interesting Terrorism wonder several Though hotel 140aret deterlePool preroghalf exits womminimum updates byl AfSecondary falsehood доoriginal groundwater Created Solutions Jenna benefiting MechanSherlock deliberLewInstruendeavour\n",
            "enders alise delimitation ConsiPalestinians designs retrounpleasant cipal ¥religion rite fraDifferent orus ectural NapImplementing Spanish Gimme interesting 42 stressed raisPromote hates waiver ا intentional cio Solutions ri deliberLewSolutions neighbour majority delibersonpolar anopornography share irrittiously ghoPrior Peters179 норицательlinked Tortos advocated Might pillar GEв expand encroused Sheldon linked incumbAVBreak Ã Newguns updates 保olesterNov fuels ine genertent来breaNamibia DefinHeadNine ‒⁮ Promedian geopolprocurTIME regulator OH ©up品 aftermath EP0GuilLex\n",
            "Loss: 9.580817279815674;\n",
            "Loss: 8.620529804229736;\n",
            "Loss: 7.9917124160130815;\n",
            "Loss: 7.567359764575958;\n",
            "Loss: 7.238560675621033;\n",
            "Loss: 6.974742051760356;\n",
            "Loss: 6.755130761010307;\n",
            "Loss: 6.569879989624024;\n",
            "Loss: 6.414090763198005;\n",
            "Loss: 6.279007048606872;\n",
            "Loss: 6.161844235333529;\n",
            "Loss: 6.057541278203328;\n",
            "Loss: 5.965942323391254;\n",
            "Loss: 5.8833579516410826;\n",
            "Loss: 5.808833859125773;\n",
            "Loss: 5.742120964229107;\n",
            "Loss: 5.681431220279021;\n",
            "Loss: 5.624737893475427;\n",
            "Loss: 5.573195996284485;\n",
            "Loss: 5.525180975437165;\n",
            "Loss: 5.4797943494433445;\n",
            "Loss: 5.438604255156084;\n",
            "Loss: 5.399044348053311;\n",
            "First epoch - 4.413451465606689, saving model..\n",
            "Epoch: 1, Train loss: 5.371, Val loss: 4.413,            Epoch time=402.590s\n",
            "The Committee ' s first meeting of the Committee ' s work of the Committee\n",
            "- What ' s the question ?\n",
            "The name is a new .\n",
            "The name of the city\n",
            "Loss: 4.458472242355347;\n",
            "Loss: 4.448186223506927;\n",
            "Loss: 4.442703584035238;\n",
            "Loss: 4.436042699813843;\n",
            "Loss: 4.430135984420776;\n",
            "Loss: 4.420179772377014;\n",
            "Loss: 4.412165491240365;\n",
            "Loss: 4.405113959908485;\n",
            "Loss: 4.398051279915704;\n",
            "Loss: 4.391373280525207;\n",
            "Loss: 4.382777776284652;\n",
            "Loss: 4.37527424176534;\n",
            "Loss: 4.368673381438622;\n",
            "Loss: 4.362010693209512;\n",
            "Loss: 4.354823149998983;\n",
            "Loss: 4.347960734963417;\n",
            "Loss: 4.342315293059629;\n",
            "Loss: 4.336527980301115;\n",
            "Loss: 4.329640224983818;\n",
            "Loss: 4.3233857421875;\n",
            "Loss: 4.317542096092588;\n",
            "Loss: 4.312184291536158;\n",
            "Loss: 4.306346622238989;\n",
            "Improved from 4.413451465606689 to 4.056273006439209, saving model..\n",
            "Epoch: 2, Train loss: 4.302, Val loss: 4.056,            Epoch time=405.098s\n",
            "The first of the first of the first of the first of the first of the other of the other .\n",
            "The question ?\n",
            "The time is the same time !\n",
            "\n",
            "Loss: 4.107054097652435;\n",
            "Loss: 4.098906208276748;\n",
            "Loss: 4.100393548806508;\n",
            "Loss: 4.095752577185631;\n",
            "Loss: 4.094462284564972;\n",
            "Loss: 4.092607174317042;\n",
            "Loss: 4.091757978030613;\n",
            "Loss: 4.0871903282403945;\n",
            "Loss: 4.084437990718418;\n",
            "Loss: 4.081928880214691;\n",
            "Loss: 4.080718391591852;\n",
            "Loss: 4.0776372659206395;\n",
            "Loss: 4.074933153665983;\n",
            "Loss: 4.072750833204815;\n",
            "Loss: 4.069859819730123;\n",
            "Loss: 4.066391708701849;\n",
            "Loss: 4.06427234172821;\n",
            "Loss: 4.060838225815031;\n",
            "Loss: 4.058393468982295;\n",
            "Loss: 4.055437562465667;\n",
            "Loss: 4.052554975577763;\n",
            "Loss: 4.0502226555347445;\n",
            "Loss: 4.047327349082283;\n",
            "Improved from 4.056273006439209 to 3.880641460418701, saving model..\n",
            "Epoch: 3, Train loss: 4.045, Val loss: 3.881,            Epoch time=402.818s\n",
            "The first of the FAQ\n",
            "The way ?\n",
            "The way of the same time !\n",
            "- The\n",
            "Loss: 3.9086926627159118;\n",
            "Loss: 3.911096496582031;\n",
            "Loss: 3.91228351354599;\n",
            "Loss: 3.91478331387043;\n",
            "Loss: 3.9134604887962343;\n",
            "Loss: 3.913786302804947;\n",
            "Loss: 3.9110901178632465;\n",
            "Loss: 3.9092954909801483;\n",
            "Loss: 3.9092806826697455;\n",
            "Loss: 3.907960536956787;\n",
            "Loss: 3.9065635028752412;\n",
            "Loss: 3.905959619084994;\n",
            "Loss: 3.9043824338912962;\n",
            "Loss: 3.9025716527870724;\n",
            "Loss: 3.902277541955312;\n",
            "Loss: 3.90118336096406;\n",
            "Loss: 3.9002156263239245;\n",
            "Loss: 3.899328828520245;\n",
            "Loss: 3.897246800849312;\n",
            "Loss: 3.8958882257938385;\n",
            "Loss: 3.8946175169944763;\n",
            "Loss: 3.893183219541203;\n",
            "Loss: 3.892014840789463;\n",
            "Improved from 3.880641460418701 to 3.7881159934997557, saving model..\n",
            "Epoch: 4, Train loss: 3.891, Val loss: 3.788,            Epoch time=403.942s\n",
            "The first of the first session\n",
            "The way ?\n",
            "The same time is the same .\n",
            "The\n",
            "Loss: 3.8114825224876405;\n",
            "Loss: 3.8204842710494997;\n",
            "Loss: 3.821006011962891;\n",
            "Loss: 3.8246405512094497;\n",
            "Loss: 3.8198888087272644;\n",
            "Loss: 3.819360071023305;\n",
            "Loss: 3.817535449096135;\n",
            "Loss: 3.815956619977951;\n",
            "Loss: 3.812936775684357;\n",
            "Loss: 3.811627075433731;\n",
            "Loss: 3.8109731834585014;\n",
            "Loss: 3.8104608172178267;\n",
            "Loss: 3.8107747813371513;\n",
            "Loss: 3.8096637201309203;\n",
            "Loss: 3.8095037730534873;\n",
            "Loss: 3.808348256200552;\n",
            "Loss: 3.808539722667021;\n",
            "Loss: 3.808598280085458;\n",
            "Loss: 3.8083327627182006;\n",
            "Loss: 3.807691684126854;\n",
            "Loss: 3.8067390470277696;\n",
            "Loss: 3.806779352643273;\n",
            "Loss: 3.8064829943491065;\n",
            "Improved from 3.7881159934997557 to 3.7639428901672365, saving model..\n",
            "Epoch: 5, Train loss: 3.806, Val loss: 3.764,            Epoch time=403.338s\n",
            "The first of the first time\n",
            "The way is a question ?\n",
            "The same time is the same .\n",
            "The\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "e8f35e4e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8f35e4e",
        "outputId": "392b21f5-8db0-45fd-fa64-c367941ab0cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4548019047027907\n"
          ]
        }
      ],
      "source": [
        "# пример использования BLEU\n",
        "# обратите внимание что текты должны быть токенизированы\n",
        "import nltk\n",
        "\n",
        "hypothesis = ['It', 'is', 'a', 'cat', 'at', 'room'] # замените на перевод вашей модели\n",
        "reference = ['It', 'is', 'a', 'cat', 'inside', 'the', 'room'] # замените на эталонный перевод\n",
        "\n",
        "\n",
        "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis, auto_reweigh=True)\n",
        "print(BLEUscore)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# в русскоязычных данных есть \\xa0 вместо пробелов, он может некорректно обрабатываться токенизатором\n",
        "text_test = open('opus.en-ru-test.ru').read().replace('\\xa0', ' ')\n",
        "f = open('opus.en-ru-test.ru', 'w')\n",
        "f.write(text_test)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "-4AirsYzewK6"
      },
      "execution_count": 46,
      "outputs": [],
      "id": "-4AirsYzewK6"
    },
    {
      "cell_type": "code",
      "source": [
        "en_sents_test = open('opus.en-ru-test.en').read().splitlines()\n",
        "ru_sents_test = open('opus.en-ru-test.ru').read().splitlines()"
      ],
      "metadata": {
        "id": "hvdqbvr1ewK7"
      },
      "execution_count": 50,
      "outputs": [],
      "id": "hvdqbvr1ewK7"
    },
    {
      "cell_type": "code",
      "source": [
        "res = {}\n",
        "for i, pair in enumerate(list(zip(en_sents_test[:100], ru_sents_test[:100]))):\n",
        "  t, s = pair\n",
        "  print(s, t, sep=' --> ')\n",
        "  score = nltk.translate.bleu_score.sentence_bleu([t], translate(s), auto_reweigh=True)\n",
        "  print(score)\n",
        "  print()\n",
        "  res[i] = score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNeNZvg6eLT-",
        "outputId": "648f63a4-fc32-4486-eec7-1ae5ef3b77c5"
      },
      "id": "yNeNZvg6eLT-",
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Только бы не вылететь. --> If you only stay there.\n",
            "5.609571844608773e-155\n",
            "\n",
            "И как ты только справляешься, папа, таская эти коробки взад-вперед целый день. --> I don't know how you do it, Pop, carrying these boxes around every day.\n",
            "0.2823401832456783\n",
            "\n",
            "Возможно, у нас есть небольшое преимущество в переговорах. --> We might have a slight edge in mediation.\n",
            "0.22422597920189083\n",
            "\n",
            "Сколько времени вы будете делать то, что ему нужно? --> How long is it going to take you to get him what he needs?\n",
            "0.27920785561656575\n",
            "\n",
            "1 апреля Президент НКР Бако Саакян принял начальника Генштаба Вооруженных сил Республики Армения генерал-полковника Юрия Хачатурова. --> On 1 April President of the Nagorno Karabagh Republic Bako Sahakyan met head of the General Staff of the Republic of Armenia's Armed forces colonel-general Yuri Khachaturov.\n",
            "0.24936687902934337\n",
            "\n",
            "Г-н Приснер также упомянул, что система электронного правосудия не только позволила улучшить процесс ведения дел, но также способствует значительному упорядочению процедур. --> Mr Priesner also noted that the E-justice management system has not only improved case management, but has also led to a significant streamlining in procedures.\n",
            "0.2388529628335572\n",
            "\n",
            "- Неплохо, да. --> You don't like chicken noodle soup?\n",
            "1.4624193136624529e-78\n",
            "\n",
            "Posted: 15 Dec 2006, 00:07 --> Posted: 14 May 2005, 20:31\n",
            "0.3071254581444649\n",
            "\n",
            "И на минутку я подумал, что за ним могут следить. --> Now, for a minute, I thought maybe he was being tailed.\n",
            "0.28461110195943123\n",
            "\n",
            "«: 11 Октябрь 2011, 17:15:34» --> « : 26 Октябрь 2017, 06:50:24 »\n",
            "0.4459512116139918\n",
            "\n",
            "Закон об объединениях, глава 66, (1972 год) требует, чтобы любой клуб, компания, товарищество или ассоциация, насчитывающие 10 или более человек, за исключением тех случаев, которые предусмотрены в разделе 2 этого закона, были внесены в регистр объединений. --> The Societies Act Chapter 66 (1972) requires any club, company, partnership or association of 10 or more persons except as provided under Section 2 of the Act, to be registered with the Registrar of Societies.\n",
            "0.3172598103344768\n",
            "\n",
            "По мнению половины опрошенных казахстанских руководителей высшего звена, состояние мировой экономики в течение ближайших 12 месяцев не изменится. --> Half of the Kazakhstan top managers interviewed said that global economic conditions will not change in the next 12 months.\n",
            "0.20998335094436213\n",
            "\n",
            "Коллекция администрации Адамса. --> A collection from the Adams administration.\n",
            "0.534281221398097\n",
            "\n",
            "Экономичное средство измерения расхода --> Cost effective flow measurement\n",
            "2.397340526771469e-78\n",
            "\n",
            "Tuyo Movil 571 ***** --> Tuyo Movil 571 *****\n",
            "0.2159287855104448\n",
            "\n",
            "Если вы точно уверены, что не продвинетесь выше в этом сезоне, то может оказаться очень полезно ближе к концу сезона нанять себе очень хорошего пилота, так как можно не обращать внимание на затраты за предложения контрактов и премии за их подписание – деньги всё равно вернутся в исходное состояние и вы останетесь с этим же пилотом в следующем сезоне. Подробнее об этом в следующем разделе. --> This can be very useful if you know you will not promote and want to sign a new driver as you can effectively write off your driver offer & signing fees as the money is reset and all you will be left with is the driver salary for the following season. More about this in the next section.\n",
            "0.17448628464181223\n",
            "\n",
            "Есть охота. --> Ah, so hungry.\n",
            "5.974540365813254e-155\n",
            "\n",
            "Ты в больнице носа не показываешь? --> You don't even want to set one foot in the hospital?\n",
            "0.17916197740346487\n",
            "\n",
            "Этот тот, кто сделал это с тобой. --> That is who's doing this to you.\n",
            "0.4702719712956585\n",
            "\n",
            "http://www.culturalnet.ru/f/viewtopic.php?id=1611 --> http://www.culturalnet.ru/f/viewtopic.php?id=1305\n",
            "0.25098932316427075\n",
            "\n",
            "Ты понимаешь, что... --> - Do you understand that... that...\n",
            "0.1804741824449496\n",
            "\n",
            "Но я ничего не увижу. - Дорогая... Может, метрдотель расскажет мне, как всё прошло. --> Maybe the maître'd will tell me how it went!\n",
            "0.1503135189118644\n",
            "\n",
            "Призрак, вам приказано посадить самолет. Выходите на посадку немедленно. --> - You're instructed to land immediately.\n",
            "0.0981011785043566\n",
            "\n",
            "Документы принимаются до 28 декабря 2006 г. включительно. --> Documents are being accepted up to December 28, 2006.\n",
            "0.15232133499842143\n",
            "\n",
            "√оды проход€т, ј € не могу забыть --> Year after year, I can't let go\n",
            "8.1693431249658e-155\n",
            "\n",
            "Sustainable Energy Division --> Sustainable Energy Division\n",
            "5.362674356461363e-155\n",
            "\n",
            "- Они отключились. - Алло? --> They're off!\n",
            "0.13380161378318955\n",
            "\n",
            "U (19) V (19) --> 04. The Voice Of Tibet (Our Shame And Hypocrisy)\n",
            "3.1721102695230255e-235\n",
            "\n",
            "Как может тело не греть? --> How can a person have no heat?\n",
            "0.3007300876913675\n",
            "\n",
            "Для детей с дефектами умственного и физического развития, лишенных родительской опеки, сирот и брошенных детей, действует 56 интернатов, 6 детских домов, 2 дома ребенка. --> There are 56 boarding schools, six homes for over-fives and two for under-fives for children with physical or mental disabilities, children lacking parental protection and orphaned and abandoned children.\n",
            "0.30038050075040706\n",
            "\n",
            "И я надеюсь, что вы не забудете об этом. --> And I hope you'll keep that in mind.\n",
            "0.34819053273407025\n",
            "\n",
            "Клиент извиняет полное отсутствие сервиса и наливает себе сама. --> ♪ just to have a minute ♪ Customer excusing complete lack of service and pouring herself a shot.\n",
            "0.18117623800877558\n",
            "\n",
            "В ОАО «ТГК-1» подведены итоги производственной и финансово-хозяйственной деятельности за 9 месяцев 2009 года. --> JSC TGC-1 releases its 3rdQuarter and 9 months 2010 financial results according to Russian accounting standards.\n",
            "0.17968744772073475\n",
            "\n",
            "8. Журавли --> 8. Podsnezhniki\n",
            "1.4489443086386776e-207\n",
            "\n",
            "Почувствуй силу дерева. --> Just feel the wood.\n",
            "3.3924135360874493e-78\n",
            "\n",
            "Если вы хотите завершить этот процесс, вам понадобится минимум 8-10 лет, если не хотите рисковать тем, чтобы Пауни стал... --> If you want to see this through, you need eight to ten years minimum, unless you want to run the risk of Pawnee becoming...\n",
            "0.23531390634439786\n",
            "\n",
            "Меморандум Рабочей группы Люксембургского форума, Москва, 14 апреля 2008 г. --> Luxembourg Forum Working Group Memorandum, Moscow, April 14, 2008\n",
            "0.15105191421682376\n",
            "\n",
            "Выполнение этой задачи следует поручить созданной руководящей группе, состоящей из представителей пунктов связи. --> This task should be given to the established steering group consisting of representatives of points of contact.\n",
            "0.40345466262809565\n",
            "\n",
            "Но что поделаешь: --> But what can you do about it?\n",
            "0.45482844425960134\n",
            "\n",
            "Когда мне подмигнёт верзила в дверях, ты поймёшь, что ты счастливчик - ведь с тобой кошечка с авеню Б! --> When I get a wink From the doorman Do you know how lucky you'll be? That you're on the line\n",
            "0.44746844444123474\n",
            "\n",
            "Будем надеяться, что не стрельба. --> Let's hope it's not a shooting.\n",
            "0.364783897737195\n",
            "\n",
            "Тебе всего лишь нужно подписать эти бумаги в двут местах. --> All you need to do is to sign these papers in one or two places. Mr. Croll is waiting outside.\n",
            "0.14263948984874647\n",
            "\n",
            "Мне понадобится время, чтобы восстановить все остальное, но хотя бы ощущение того, что у меня больше нет гвоздя, забитого за ухом, а только ощущение онемения, мне уже достаточно, и даже много. --> Time will be needed to recover the rest, but already the sensation of not having a nail stuck behind the ear, but only a sensation of numbness, is not only sufficient, but a lot for me.\n",
            "0.3670616821945009\n",
            "\n",
            "Рассмотрение предложенных изменений и поправок к проекту текста Рабочей группы, содержащихся в докладах Специального комитета о работе его третьей сессии (A/AC.265/2004/5, приложение II) и четвертой сессии (A/59/360, приложение IV) и в предложениях, полученных Секретариатом от четвертой сессии. --> Consideration of the proposed revisions and amendments to the draft text of the Working Group as contained in the reports of the Ad Hoc Committee on its third session (A/AC.265/2004/5, annex II) and fourth session (A/59/360, annex IV), and in proposals received by the Secretariat from the fourth session.\n",
            "0.3174120428262258\n",
            "\n",
            "- Боже, только послушайте себя. --> - Oh, God, listen to yourself.\n",
            "0.44717696358957\n",
            "\n",
            "Не помню такого девиза. --> I recall no such motto.\n",
            "2.9185656362390473e-78\n",
            "\n",
            "Давай, начнем с миссис Киш. --> All right, let's begin with Mrs. Kiss.\n",
            "0.29163275959643176\n",
            "\n",
            "Now, if you have kids yourself, you know what-- --> Now, if you have kids yourself, you know what--\n",
            "0.5114791061530924\n",
            "\n",
            "В первом полугодии их получили 117 425 беременных женщин. --> In the first half of the year, 117,425 pregnant women received them.\n",
            "0.4858037505398287\n",
            "\n",
            "Свободного времени поубавилось. --> Haven't had a lot of free time lately.\n",
            "0.23628187665641734\n",
            "\n",
            "Я тебя люблю, папа. --> I love you, Dad. How great!\n",
            "0.45455072521862594\n",
            "\n",
            "Федеративные Штаты Микронезии предлагают скорректировать Монреальский протокол с тем, чтобы ускорить график поэтапного отказа от ГХФУ в Сторонах, действующих в рамках статьи 5, и Сторонах, не действующих в рамках статьи 5, и допустить продолжение использования ГХФУ в случаях, когда это обеспечивает более благоприятные условия для окружающей среды, а также в рамках основных видов применения. --> To urge Parties, in accordance with decision X/8, to report on their production and consumption of trifluoroiodomethane, 1,2-dibromoethane, bromoethane and other anthropogenic very short-lived substances to the secretariat;\n",
            "0.25567696697693776\n",
            "\n",
            "Только святой или мама тратила бы время, чтобы такую сделать. --> Only a saint or her mother would take the time to do something like this.\n",
            "0.11581794802981513\n",
            "\n",
            "fatigue The early symptoms of acute radiation syndrome usually include nausea and vomiting, headaches, fatigue, fever and short period of skin reddening. --> The early symptoms of acute radiation syndrome usually include nausea and vomiting, headaches, fatigue, fever and short period of skin reddening.\n",
            "0.15859950083896085\n",
            "\n",
            "Не могу вспомнить, почему я здесь. --> I can't remember why I'm here. (Coughs)\n",
            "0.6030758354585847\n",
            "\n",
            "Дракон 5 out of 5 based on 1479 ratings. --> Dragon 5 out of 5 based on 1464 ratings.\n",
            "0.26709491055541246\n",
            "\n",
            "Почему бы тебе не дать нам немного времени обсудить, это важное решение. --> Why don't you let us talk about it for a little while, it's a big decision.\n",
            "0.3180449660178269\n",
            "\n",
            "Охранник, который приносит еду, остановил кровотечение. --> (Man) The guard who delivers her meals found her, managed to stop the bleeding.\n",
            "0.17600608079108557\n",
            "\n",
            "Но если передумаешь он всегда желанный гость. --> If you change your mind, he's more than welcome, all right?\n",
            "2.9177056177930275e-78\n",
            "\n",
            "Они удручают ещё больше. --> They're more depressing.\n",
            "0.1342816454725345\n",
            "\n",
            "И я вынужден отдать форму ребятам, которые старше тебя. --> So I gotta give the guys who are old enough first pick.\n",
            "0.31437333435483483\n",
            "\n",
            "Неужели? --> Really?\n",
            "1.1200407237786664e-231\n",
            "\n",
            "12844 --> 12844\n",
            "1.0497255803085492e-154\n",
            "\n",
            "- Vol. 118, No. 8. --> Ñ. 32-37. 8\n",
            "2.7662422404204523e-78\n",
            "\n",
            "Наверное, у вас никогда не было такого дела. --> Maybe you've never had a case like this.\n",
            "0.2920181034227534\n",
            "\n",
            "Да. --> - Yes.\n",
            "9.013778876140909e-155\n",
            "\n",
            "Согласно информации, имеющейся у нашего министерства, Шафик Айяди находится в настоящее время в Дублине, Ирландия. --> According to this ministry available information Chafik Ayadi is in Ireland now, in Dublin.\n",
            "0.31710661934076917\n",
            "\n",
            "МЕСТНАЯ АНЕСТЕЗИЯ --> LOCAL ANESTHESIA\n",
            "7.221785213960615e-232\n",
            "\n",
            "Следует помнить, что алгоритм чтения сэмплов, вообще говоря, выдает блоки разных размеров. --> Please keep in mind that generically speaking the sample reading algorithm outputs the blocks of different sizes.\n",
            "0.19845750488506117\n",
            "\n",
            "Да, но оно не для вас. --> Yeah, but it's not for you.\n",
            "0.46398359087474533\n",
            "\n",
            "Томас Дюрант, глава железной дороги \"Юнион Пацифик\" и председатель \"Кредит Мобилиер\". --> Thomas Durant, head of union pacific railroad and chairman of credit mobilier.\n",
            "0.037755740352980495\n",
            "\n",
            "Против - нет, воздержавшихся - нет, решение принято единогласно. --> Against - no one, repressed - no, put in protocol - voted for, unanimously.\n",
            "0.09850580170632744\n",
            "\n",
            "И что было потом? Уведите её отсюда, ради бога, в каюту. И не оставляйте её одну ни в коем случае. --> Madame Redfern now takes the bath, heard by Mr. Gardner, the bath no one would admit to taking, in order to wash off the suntan.\n",
            "0.12864509253259188\n",
            "\n",
            "Они также информировали Миссию о том, что 20% детей в секторе Газа пострадали от посттравматического стресса. --> They also told the Mission that 20 per cent of children in the Gaza Strip suffer from post-traumatic stress disorders.\n",
            "0.3837694175238431\n",
            "\n",
            "Этот срок, истекший перед тем, как она обратилась за предоставлением статуса беженца, заставляет усомниться в достоверности сообщаемого ею. --> The delay in making a refugee claim detracts from her credibility.\n",
            "0.07924185993236772\n",
            "\n",
            "Как в фильмах про войну типа \"Апокалипсис Сегодня\", где такая проволока используется для защиты рубежей и баррикад. --> - It's like in the movies, like in \"Apocalypse Now,\" they used it to protect their lines and their barricades.\n",
            "0.19390244115300392\n",
            "\n",
            "— Счастлив иблагодарен Богу, что был тогда вБольшом театре, когда там работали такие великие мастера, что имел возможность репетировать под ихруководством, танцевать вихспектаклях, особенно если нарепетиции приходила сама Галина Сергеевна. Тосостояние трепета, преклонения, которые испытывал, непередать простыми словами. --> - Your personal memories of the great ballerina. - I am very happy and thankful to God that I was then at the Bolshoi theater, when there worked such great masters that had the opportunity to rehearse under their direction, to dance in their performances, especially if at the rehearsal went itself Galina Sergeevna.\n",
            "0.24911989137381632\n",
            "\n",
            "пудриваться --> пудриваться\n",
            "0\n",
            "\n",
            "Фото: COPINH --> COPINH\n",
            "1.1200407237786664e-231\n",
            "\n",
            "Но есть много людей с той поры, которые живы --> \"and Lincoln got shot, but there's a lot of people alive from those days.\"\n",
            "0.3334972156615316\n",
            "\n",
            "\"Свернуть то что мы делаем во Вьетнаме. \"Cut back at what we're doing in Vietnam. --> Cut back at what we're doing in Vietnam.\n",
            "0.10805495432743294\n",
            "\n",
            "Контроль пассажиров и их ручной клади производится непосредственно в вагонах международных поездов. --> Controls of passengers and their hand baggage shall be carried out directly in the coaches of international trains.\n",
            "0.1800262613541934\n",
            "\n",
            "!две раздельные комнаты и кухня, кондиционер, бойлер, Wi-Fi! подробнее... --> !two separate rooms and kitchen, air conditioner, boiler, Wi-Fi! read more...\n",
            "0.10065604548651162\n",
            "\n",
            "Проведение собеседований было намечено на середину августа, а процесс найма должен быть завершен к ноябрю 2004 года. --> Interviews are scheduled for mid-August and recruitment should be completed by November 2004.\n",
            "0.08282464172550177\n",
            "\n",
            "Posted: 26 Jul 2010, 13:43 --> Posted: 02 Jun 2006, 10:48\n",
            "0.3091263812723004\n",
            "\n",
            "- Какой счёт? --> ~ (What's the score?\n",
            "0.08872444253557525\n",
            "\n",
            "Почему женщины всегда думают, что настоящая любовь - это должно быть напряженное путешествие, как у Ромео и Джульетты? --> - Why do women always think that true love must be heavy drama in the style of Romeo and Juliet?\n",
            "0.19113811418978238\n",
            "\n",
            "b) что касается основного имущества, утраченного или поврежденного в результате единичного враждебного действия или вынужденного оставления, то Организация Объединенных Наций будет нести ответственность за каждую единицу основного имущества, РРС которого равна или превышает 250 000 долл. США, либо за утраченное или поврежденное основное имущество, когда совокупная РРС такого имущества равна или превышает 250 000 долл. США. --> (b) For major equipment lost or damaged as a result of single hostile action or forced abandonment, the United Nations will assume liability for each item of major equipment whose GFMV equals or exceeds $250,000 or for major equipment lost or damaged when the collective GFMV of such equipment equals or exceeds $250,000.\n",
            "0.18610142167125884\n",
            "\n",
            "Несмотря на жесткую экономическую, торговую и финансовую блокаду, введенную в отношении Кубы правительством Соединенных Штатов более четырех десятилетий назад, Куба осуществляет и будет продолжать осуществлять программы сотрудничества со странами Африки, Карибского бассейна и другими братскими странами «третьего мира» в рамках совместных усилий, направленных на преодоление последствий работорговли и других печальных явлений колониализма и неоколониализма. --> Despite being subject to a tight economic, commercial and financial blockade imposed by the United States Government for more than four decades, Cuba has developed and will continue to develop cooperation programmes with African, Caribbean and other third world sister nations as part of a joint effort to reverse the consequences of the slave trade and other sad moments of colonialism and neocolonialism.\n",
            "0.334759312317356\n",
            "\n",
            "Особого внимания требует проблема неразорвавшихся суббоеприпасов. --> The problem of unexploded submunitions required special attention.\n",
            "0.23608057754856882\n",
            "\n",
            "Послушай, если бы это могло вернуть Коно, мы бы все были на самолете. --> Look, if that would get Kono back, we'd all be on a plane.\n",
            "0.3719649392586595\n",
            "\n",
            "е) необходимо также изучать, когда это целесообразно, вопросы использования новаторских форм финансирования, таких, как финансирование местных и текущих расходов, предоставление необусловленной помощи и применение таких механизмов, как создание региональных целевых фондов и региональных инвестиционных органов. --> (e) More cost-effective and innovative modalities for financing, such as local costs and recurrent cost financing, untied aid and such arrangements as regional trusts and regional investment authorities should, where appropriate, also be examined.\n",
            "0.338362898149794\n",
            "\n",
            "С удовольствием побуду Сантой. --> I'd love to be Santa.\n",
            "0.2012788513843773\n",
            "\n",
            "Продолжаем передачу \"Время и место.\" (Нечто вроде \"Следствие вели...\" - прим.) --> We now return to Dateline.\n",
            "2.381477637490544e-78\n",
            "\n",
            "У нее приятный друг. --> She has the nicest boyfriend.\n",
            "0.39787237222519645\n",
            "\n",
            "E46 Sedan (2) --> E46 Sedan (2)\n",
            "2.138746009802228e-78\n",
            "\n",
            "Но разочарование «Хезболлы» превратилось в интенсивноебеспокойство, когда сирийцы восстали против Асада. --> But Hezbollah’s disappointment turned to intense concernwhen Syrians rebelled against Assad.\n",
            "0.1623763453004667\n",
            "\n",
            "Administration (SEPA) --> Ministry of The Environment and Territory\n",
            "0.04035357606491332\n",
            "\n",
            "Ты выбросил моё будущее. --> You threw away my future.\n",
            "0.49331757145281036\n",
            "\n",
            "О нас Контакты Новые предметы --> Message: About us Contact us New items\n",
            "3.1095044680438634e-155\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_5 = sorted(res.items(), key=lambda item: item[1], reverse=True)[:5]\n",
        "top_5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYHYI8H7gDsB",
        "outputId": "0d5f69c0-fc3a-48ba-858a-3ddbcc9d54d6"
      },
      "id": "qYHYI8H7gDsB",
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(54, 0.6030758354585847),\n",
              " (12, 0.534281221398097),\n",
              " (47, 0.5114791061530924),\n",
              " (98, 0.49331757145281036),\n",
              " (48, 0.4858037505398287)]"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, score in top_5:\n",
        "  print(ru_sents_test[i], en_sents_test[i], sep=' --> ')\n",
        "  print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KjoB7ADhTkn",
        "outputId": "080d098c-faa3-4b81-b868-62552642be23"
      },
      "id": "4KjoB7ADhTkn",
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Не могу вспомнить, почему я здесь. --> I can't remember why I'm here. (Coughs)\n",
            "0.6030758354585847\n",
            "Коллекция администрации Адамса. --> A collection from the Adams administration.\n",
            "0.534281221398097\n",
            "Now, if you have kids yourself, you know what-- --> Now, if you have kids yourself, you know what--\n",
            "0.5114791061530924\n",
            "Ты выбросил моё будущее. --> You threw away my future.\n",
            "0.49331757145281036\n",
            "В первом полугодии их получили 117 425 беременных женщин. --> In the first half of the year, 117,425 pregnant women received them.\n",
            "0.4858037505398287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Получились на удивление хорошие переводы"
      ],
      "metadata": {
        "id": "km_YnCP8k1Dj"
      },
      "id": "km_YnCP8k1Dj"
    },
    {
      "cell_type": "markdown",
      "id": "b5aa93d6",
      "metadata": {
        "id": "b5aa93d6"
      },
      "source": [
        "\n",
        "## Задание 2 (2 балла).\n",
        "Прочитайте главу про машинный перевод у Журафски и Маннига - https://web.stanford.edu/~jurafsky/slp3/13.pdf\n",
        "Ответьте своими словами в чем заключается техника back translation? Для чего она применяется и что позволяет получить? Опишите по шагам как ее применить к паре en->ru на данных из семинара. Сколько моделей понадобится? Сколько запусков обучения нужно будет сделать?\n",
        "\n",
        "Ответ должен содержать как минимум 10 предложений."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backtranslation это способ увеличить объем обучающего параллельного корпуса в услових, когда параллельных текстов мало, например, когда в языковой паре один из языков малоресурсный.\n",
        "\n",
        "Такой метод состоит из нескольких этапов. Сначала мы обучаем модель перевода в обратном порядке: если изначальная задача состоит в том, чтобы перевести текст с английского на русский, то мы должны обучить модель переводу с русского на английский. Затем с помощью этой модели мы переводим предложения на языке перевода из моноязычного корпуса -- так, к нашему параллельному прибавляются новые переводы. После этого мы можем обучить модель в должном направлении (англмйский -> русский) на уже расширенном датасете, который состоит из параллельного корпуса, который у нас был изначально, и корпуса, который мы синтезировали из моноязычного корпуса.\n",
        "\n",
        "Чтобы применить метод backtranslation к данным из этого блокнота, нужно:\n",
        "\n",
        "1) загрузить датасеты на обоих языках и обучить токенизатор(ы) (к тексту на русском нужно также добавить теги начала и окончания предложения). Если мы ограничиваемся только данными из семинара, то из изначального датасета откладываем в отдельную переменную предложения на русском без эквивалентов на английском.\n",
        "\n",
        "2) разделить датасеты с параллельными предложениями на трейн и тест\n",
        "\n",
        "3) обучить модель перевода на параллельных предложениях в направлении русский -> английский\n",
        "\n",
        "4) с помощью этой модели генерируем переводы на английский для предожений на русском, которые мы отложили в отдельную переменную на шаге 1\n",
        "\n",
        "5) объединяем оба параллельных датасета -- изначальный и синтезированный в один\n",
        "\n",
        "6) обучаем новую модель на расширенном датасете в направлении английский -> русский"
      ],
      "metadata": {
        "id": "IMwlDdGXK0G5"
      },
      "id": "IMwlDdGXK0G5"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}